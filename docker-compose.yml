version: '3.8'

networks:
  lakehouse: {}

volumes:
  minio-data: {}
  pg-data: {}

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    networks: [lakehouse]
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    networks: [lakehouse]
    depends_on: [zookeeper]
    ports: ['9092:9092']
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'


  minio:
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    networks: [lakehouse]
    command: server /data --console-address ':9001'
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    volumes:
      - minio-data:/data
    ports:
      - '9000:9000'
      - '9001:9001'

  minio-mc:
    image: minio/mc:RELEASE.2024-12-18T11-00-42Z
    networks: [lakehouse]
    depends_on: [minio]
    entrypoint: ["/bin/sh","-c"]
    command: |
      "mc alias set local http://minio:9000 minio minio123 && \
       mc mb -p local/lakehouse || true && \
       mc anonymous set download local/lakehouse || true && \
       tail -f /dev/null"

  postgres:
    image: postgres:16
    networks: [lakehouse]
    environment:
      POSTGRES_USER: lakehouse
      POSTGRES_PASSWORD: lakehouse
      POSTGRES_DB: lakehouse
    volumes:
      - pg-data:/var/lib/postgresql/data
    ports:
      - '5432:5432'


  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    networks: [lakehouse]
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
    ports:
      - '8080:8080'
      - '7077:7077'


  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile
    networks: [lakehouse]
    depends_on: [spark-master]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    ports:
      - '8081:8081'





  spark-bronze-stream:
    build:
      context: ./spark
      dockerfile: Dockerfile
    networks: [lakehouse]
    depends_on: [kafka, minio-mc, spark-master]
    environment:
      SPARK_MASTER: spark://spark-master:7077
      KAFKA_BOOTSTRAP: kafka:29092
      MINIO_ENDPOINT: http://minio:9000
      MINIO_ACCESS_KEY: minio
      MINIO_SECRET_KEY: minio123
      DELTA_BUCKET: lakehouse
    entrypoint: ["/opt/spark/bin/spark-submit"]
    command:
      - --master
      - spark://spark-master:7077
      - --packages
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,io.delta:delta-spark_2.12:3.1.0,org.apache.hadoop:hadoop-aws:3.3.4
      - /app/jobs/bronze_stream.py

  producer:
    build:
      context: ./producer
      dockerfile: Dockerfile
    networks: [lakehouse]
    depends_on: [kafka]
    environment:
      KAFKA_BOOTSTRAP: kafka:29092
      KAFKA_TOPIC: hsl_stream
      HSL_MQTT_HOST: mqtt.hsl.fi
      HSL_MQTT_PORT: 8883
      HSL_MQTT_TOPIC: /hfp/v2/journey/ongoing/vp/#
    restart: unless-stopped

  grafana:
    image: grafana/grafana:11.2.0
    networks: [lakehouse]
    ports: ['3000:3000']
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - ./grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./grafana/dashboards:/var/lib/grafana/dashboards:ro

